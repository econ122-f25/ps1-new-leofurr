---
title: "Problem Set 1"
author: "Leo Furr"
date: "`r Sys.Date()`"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
library(tidyverse)
```

### Introduction

This problem set is designed to test your understanding of data wrangling concepts using the **`tidyverse`**, specifically **`dplyr`** and **`tidyr`**, as well as foundational R concepts. You'll primarily work with the `mtcars`, a simulated sales dataset, and new simulated student demographic/grade datasets, applying core verbs and functions to prepare data for analysis. This activity should take approximately 60 minutes.

### Instructions

* Read each task carefully.
* Write your R code in the provided code chunks.
* Run the code to see your output and verify your results.
* `mtcars` is a built-in R dataset (or part of the `tidyverse` installation).

---

## Part 0: R Fundamentals (10 minutes)

This section will test your understanding of basic R syntax, data types, and vector operations.

### Task 0.1: Data Types and Logical Operations

1.  Create a variable `course_name` and assign it the string `"Introduction to Data Science"`.
2.  Create a variable `num_students` and assign it the integer value `45`.
3.  Check the data type (`class()`) of `course_name` and `num_students`.
4.  Create a numeric variable `pi_value` with the value $3.14159$.
5.  Create a logical variable `is_active` and set it to `TRUE`.
6.  Evaluate the following logical expressions and print their results:
    * Is `num_students` greater than or equal to `50`?
    * Is `course_name` exactly equal to `"introduction to data science"` (case-sensitive)?

```{r}
# Your code here
# 1
course_name <- "Introduction to Data Science"
# 2 
num_students <- 45L
# 3 
class(course_name) == "character"
class(num_students) == "integer"
# 4 
pi_value <- 3.14159
#5 
is_active <- TRUE 
#6 
# is num_students >= 50?
check_num_students <- num_students >= 50 
print(check_num_students)
# is course_name exactly equal to "introduction to data science"?
check_course_name <- course_name == "introduction to data science"
print(check_course_name)
```

--- 

### Task 0.2: Working with Vectors

1.  Create a numeric vector named `temperatures` with the values $22, 25, 19, 28, 23$.
2.  Calculate the `sum()` and `mean()` of the `temperatures` vector.
3.  Add a new temperature, $30$, to the `temperatures` vector (re-assign the variable).
4.  Create a new logical vector named `is_hot` that is `TRUE` for temperatures greater than $25$ and `FALSE` otherwise.

```{r}
# Your code here

# 1 
temperatures <- c(22, 25, 19, 28, 23)
#2 
sum(temperatures)
mean(temperatures)
#3 
temperatures <- c(22, 25, 19, 28, 23, 30)
#4 
is_hot <- temperatures >= 25
print(is_hot)
```

---

## Part 1: `dplyr` Fundamentals with `mtcars` (25 minutes)

This section focuses on applying core `dplyr` verbs to the classic `mtcars` dataset. The `mtcars` dataset contains information about various car models from 1973-74.

### Task 1.1: Filtering and Arranging

1.  The `mtcars` dataset has been pre-processed for you to include `car_model` as a column.
2.  Filter the dataset to include only cars with `cyl` (number of cylinders) equal to `4` or `6` AND `mpg` (miles per gallon) greater than `20`.
3.  Arrange the results first by `mpg` in ascending order, and then by `cyl` in descending order.

```{r}
# Pre-processing step: Convert rownames to car_model
mtcars_processed <- mtcars %>%
  rownames_to_column("car_model")

mtcars
mtcars_processed

# Your code starts here: Filter and arrange mtcars_processed
# Your code here

mtcars_processed %>% filter(cyl <= 6 & mpg >= 20) %>% 
  arrange(mpg) %>% arrange(desc(cyl))

```

### Short Answer
Based on your filtered and arranged results, briefly describe the characteristics of the cars that appear at the top of your output. What does this tell you about the relationship between cylinders and miles per gallon in this subset?

Response: At the top of my output, I notice that in this subset there is not a single car with 4 cylinders that has a lower miles per gallon than a car with 6 cylinders. The relationship I notice, then, is that cars with more cylinders tend to have a lower miles per gallon. 
---

### Task 1.2: Mutating and Selecting

Using the `mtcars_processed` dataset:

1.  Create a new column `hp_per_wt` by dividing `hp` (horsepower) by `wt` (weight in 1000 lbs).
2.  Create another new column `qsec_category` based on `qsec` (1/4 mile time):
    * `"Fast"` if `qsec < 17`
    * `"Medium"` if `qsec >= 17` and `qsec < 19`
    * `"Slow"` if `qsec >= 19`
3.  Select only the `car_model`, `mpg`, `hp`, `wt`, `hp_per_wt`, and `qsec_category` columns.

```{r}
# Your code here

#1
mtcars_processed <- mtcars_processed %>% mutate(hp_per_wt = hp / wt)

#2
mtcars_processed <- mtcars_processed %>% mutate(qsec_category = case_when(qsec < 17 ~ "Fast", qsec >=17 & qsec < 19 ~ "Medium", qsec >= 19 ~ "Slow"))

#3
mtcars_processed %>% select(car_model, mpg, hp, wt, hp_per_wt, qsec_category)

```

### Short Answer
Why might creating `hp_per_wt` and `qsec_category` be useful metrics when analyzing car performance, beyond just raw horsepower and weight?

For `hp_per_wt`, having a heavier car naturally requires more horsepower to move it, so this metric allows consideration for how much horsepower a car has for their weight. And for qsec_category, being aware of very small differences in the quarter-mile time doesn't matter as much compared to being aware if they can complete it quickly or slowly, so making the variable easier to comprehend at a glance is helpful.   

---

### Task 1.3: Grouped Summaries

Using the `mtcars_processed` dataset:

1.  Group the data by `cyl` (number of cylinders) and `am` (transmission type: 0 for automatic, 1 for manual).
2.  For each group, calculate the **average `mpg`**, the **median `hp`**, and the **number of cars** (`n()`).
3.  Arrange the final result first by `cyl` (ascending) and then by `average_mpg` (descending).

```{r}
# Your code here

#1, 2, 3
mtcars_processed %>% group_by(cyl, am) %>% summarize(avg_mpg = mean(mpg), median_hp = median(hp), count = n()) %>% arrange(cyl) %>% arrange(desc(avg_mpg))
```

### Short Answer
Based on your grouped summary, what general trends do you observe regarding average `mpg` and median `hp` across different combinations of `cylinders` and `transmission` types? How do these summaries help differentiate car characteristics?

Response: The general trends I notice are that mpg tends to be higher for cars with less cylinders and for manual cars, and that horsepower tends to be higher for cars with more cylinders. A trend between horsepower and transmission types is hard to be certain about, though, because the data seems to indicate that automatic cars have more horsepower except for cars with eight cylinders. However, there are only two cars that are manual and have eight cylinders, so more data may be required for this data set. My intuition tells me that the cars in this subset may be supercars, which explains why their horsepower is so high. These summaries help differentiate car characteristics by controlling for certain variables that would make broader differentiation unhelpful. 

---

## Part 2: Reshaping and Joining Data (25 minutes)

For this part, we'll explore reshaping and joining using a simulated sales dataset and new simulated student enrollment data.

**Run this code chunk first to set up the data:**

```{r, echo = TRUE}
# Part 2 Data Setup
product_sales_wide <- tibble(
  product = c("Laptop", "Monitor", "Keyboard"),
  `2020` = c(1000, 500, 800),
  `2021` = c(1100, 550, 850),
  `2022` = c(1250, 600, 900)
) %>%
  rename(Year_2020 = `2020`, Year_2021 = `2021`, Year_2022 = `2022`) # Rename to avoid issues with non-syntactic names

# Simulated Student and Grade Data
students_demographics <- tibble(
  student_id = c("S001", "S002", "S003", "S004", "S005", "S007"), # S007 is a new student, no grades yet
  student_name = c("Alice", "Bob", "Charlie", "David", "Eve", "Frank"),
  major = c("CS", "Math", "Physics", "CS", "Biology", "History"),
  enrollment_year = c(2020, 2021, 2020, 2022, 2021, 2023)
)

student_grades <- tibble(
  student_id = c("S001", "S002", "S003", "S001", "S006", "S004"), # S006 has grades but no demographic info
  course_id = c("CS101", "MA201", "PH301", "CS102", "BI101", "CS205"),
  semester = c("Fall 2020", "Spring 2022", "Fall 2021", "Spring 2021", "Fall 2021", "Spring 2023"),
  grade_score = c(92, 85, 88, 78, 75, 90) # Assuming numeric scores for easier calculation
)

# Display the dataframes
print(product_sales_wide)
print(students_demographics)
print(student_grades)
```

---

### Task 2.1: New Variable Creation: Wide vs. Long (`product_sales` dataset)

This task demonstrates how creating new variables that depend on previous time periods is much easier in a long (tidy) format.

1.  **Analysis in Wide Format:** Using the `product_sales_wide` dataset, create new columns for the **year-over-year growth rate** for 2021 and 2022.
    * `Growth_2021`: `(Year_2021 - Year_2020) / Year_2020 * 100`
    * `Growth_2022`: `(Year_2022 - Year_2021) / Year_2021 * 100`

```{r}
# Your code here
product_sales_wide %>% mutate(Growth_2021 = (Year_2021 - Year_2020) / Year_2020 * 100, Growth_2022 = (Year_2022 - Year_2021) / Year_2021 * 100)
```

2.  **Reshape to Long Format:** Reshape `product_sales_wide` into a long format called `product_sales_long`. Pivot the year columns (`Year_2020`, `Year_2021`, `Year_2022`) into two new columns: `Year` and `Sales`. Make sure `Year` is numeric.

```{r}
# Your code here
product_sales_long <- product_sales_wide %>% pivot_longer(cols = Year_2020:Year_2022, names_to = "Year", values_to = "Sales")
product_sales_long <- product_sales_long %>% mutate(Year = case_when(Year == "Year_2020" ~ 2020, Year == "Year_2021" ~ 2021, Year == "Year_2022" ~ 2022))
print(product_sales_long)
```
3.  **Analysis in Long Format:** Using the `product_sales_long` dataset, calculate a single `Growth_Rate` column that represents the year-over-year growth for each product. You should use `group_by()` and `lag()`. The formula for growth rate is `(current_year_sales - previous_year_sales) / previous_year_sales * 100`.

```{r}
# Your code here

product_sales_long %>% group_by(product) %>% mutate(Growth_Rate = (Sales - lag(Sales)) / lag(Sales) * 100)
```

### Short Answer
Compare the code required to calculate the year-over-year growth rate in the wide format versus the long format. Which approach is more concise and scalable if you had many more years of data? Why is the long format generally preferred for this type of time-series calculation?

Response: The long-format approach is much more scalable because it only requires one function to calculate for all of the years, whereas if you took the wide-format approach, you would have to recreate the function for each year. Having the year all as one variable allows you to make calculations using the year so that, for instance, you can utilize the lag function. 

---

### Task 2.2: Mutating Joins: Student Demographics and Grades

This task explores combining student demographic information with their academic performance. Pay close attention to how different join types handle students who may or may not have corresponding grade records.

1.  **Inner Join:** Perform an `inner_join()` to combine `students_demographics` with `student_grades` based on `student_id`. This will show students who have **both** demographic information and at least one grade record.
2.  **Left Join:** Perform a `left_join()` to combine `students_demographics` (left table) with `student_grades` based on `student_id`. This will keep **all students** from the demographic data and add their grade records where available.
3.  **Advanced Mutate (after Left Join):** After performing the left join, calculate the **average `grade_score` for each student**. This will require grouping by `student_id` and `student_name`.

```{r}
# Your code for inner_join and its explanation
students_demographics %>% 
  inner_join(student_grades, by = "student_id")

# 5 rows, Eve and Frank did not make the cut because they did not take any courses, and therefore they were not included in the inner_join. The inner_join function only includes rows that have a match in both data frames.

```

```{r}
# Your code for left_join and its explanation

students_demographics %>% 
  left_join(student_grades, by = "student_id")

# 7 rows because Eve and Frank were included because the left_join includes all rows from the first data frame and substitutes NAs for any missing data. 
```

```{r}
# Your code for advanced mutate after left join

students_demographics %>% 
  left_join(student_grades, by = "student_id") %>% 
  group_by(student_id, student_name) %>% 
  mutate(avg_grade_score = mean(grade_score))

```

### Short Answer
Compare the number of rows and the content of the `inner_join()` and `left_join()` results. Specifically, identify which student(s) are present in one join but not the other, and explain why. What does the average grade calculation reveal about students with multiple grades or no grades?

Response: 

The inner_join tibble has five rows, whereas the left_join tibble has seven. The difference lies in Eve and Frank, who appear in the left_join but not the inner_join because an inner_join only combines observations that have a match in both data frames---they did not take any courses, so they do not show up in the inner_join. The left join, on the other hand, keeps all rows from the first data frame, so Eve and Frank get to stay. 
The average grade score calculation shows that for multiple grades, R seamlessly calculates it. However, if an observation has no data to calculate the mean, then the average will just be an NA. 
---

### Task 2.3: Filtering Joins: Identifying Student Enrollment Status

This task focuses on using filtering joins to identify different categories of students based on their enrollment and grade records.

1.  **Enrolled Students:** Use a `semi_join()` to identify which students from `students_demographics` are actually enrolled in and have a grade record in `student_grades`. This should return only columns from `students_demographics`.
2.  **Students Without Grades:** Use an `anti_join()` to identify which students from `students_demographics` are in the system but currently do *not* have any grade records in `student_grades` (e.g., new students, or those who haven't completed courses yet). This should return only columns from `students_demographics`.
3.  **Grades Without Students:** Use an `anti_join()` to identify any grade records in `student_grades` that do *not* correspond to an existing student in `students_demographics` (e.g., a data entry error or a record for a past student no longer in the demographic system). This should return only columns from `student_grades`.

```{r}
# Your code for question 1 (semi_join)
students_demographics %>% semi_join(student_grades, by = "student_id")
```

```{r}
# Your code for question 2 (anti_join on students_demographics)
students_demographics %>% anti_join(student_grades, by = "student_id")

```

```{r}
# Your code for question 3 (anti_join on student_grades)

student_grades %>% anti_join(students_demographics, by = "student_id")
```

### Short Answer
Describe the distinct insights gained from each of the three filtering joins in this task. How do these joins help in data validation and understanding the completeness of your student records?

Response: The insight I gained from the semi_join is that this function allowed me to match up students in the demographics list with students that are currently enrolled. Using this function will allow me to filter out unnecessary observations in the future. As for the first anti_join, this function allowed me to discover any students who were not enrolled. This tool will come in handy when, for whatever reason, there is some discrepancy that I want to spotlight. The last anti_join was useful for a similar purpose---putting to light any observations that do not match up with another data frame. 



